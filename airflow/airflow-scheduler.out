[2025-02-27T14:40:08.864-0300] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2025-02-27T14:40:08.973-0300] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-02-27T14:40:08.974-0300] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-02-27T14:40:09.028-0300] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 74312
[2025-02-27T14:40:09.030-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-27T14:40:10.024-0300] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-27T14:40:10.037-0300] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-02-27T14:40:18.266-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:18.267-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:18.268-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:18.270-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:18.271-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 47 and queue default
[2025-02-27T14:40:18.272-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:18.273-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:19.213-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:19.279-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:19.791-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:19.797-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=clear_db_locks, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:19.342782+00:00, run_end_date=2025-02-27 17:40:19.637293+00:00, run_duration=0.294511, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=121, pool=default_pool, queue=default, priority_weight=47, operator=PythonOperator, queued_dttm=2025-02-27 17:40:18.269652+00:00, queued_by_job_id=120, pid=74350
[2025-02-27T14:40:19.830-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:19.831-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:19.832-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:19.833-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:19.834-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 46 and queue default
[2025-02-27T14:40:19.834-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:19.836-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:20.719-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:20.777-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:22.153-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:22.157-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_populate_raw_orders, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:20.843892+00:00, run_end_date=2025-02-27 17:40:21.987795+00:00, run_duration=1.143903, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=122, pool=default_pool, queue=default, priority_weight=46, operator=BashOperator, queued_dttm=2025-02-27 17:40:19.832985+00:00, queued_by_job_id=120, pid=74358
[2025-02-27T14:40:22.184-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:22.185-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:22.186-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:22.187-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:22.188-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 45 and queue default
[2025-02-27T14:40:22.189-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:22.190-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:23.077-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:23.136-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:26.488-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:26.493-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_seeds, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:23.205406+00:00, run_end_date=2025-02-27 17:40:26.243570+00:00, run_duration=3.038164, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=123, pool=default_pool, queue=default, priority_weight=45, operator=BashOperator, queued_dttm=2025-02-27 17:40:22.187213+00:00, queued_by_job_id=120, pid=74366
[2025-02-27T14:40:26.524-0300] {scheduler_job_runner.py:435} INFO - 4 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:26.525-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:26.526-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:40:26.526-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:40:26.527-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 3/10 running and queued tasks
[2025-02-27T14:40:26.528-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:26.529-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:26.530-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:40:26.530-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.531-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:40:26.532-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.532-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:40:26.533-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.534-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:40:26.535-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.536-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:27.445-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:27.505-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:30.162-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:31.133-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:31.192-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:33.735-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:34.694-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:34.752-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:37.236-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:38.095-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:38.155-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:40.645-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.647-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.648-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.649-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.653-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_customers, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:34.824460+00:00, run_end_date=2025-02-27 17:40:37.054367+00:00, run_duration=2.229907, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=126, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74409
[2025-02-27T14:40:40.654-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_orders, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:27.570806+00:00, run_end_date=2025-02-27 17:40:29.970739+00:00, run_duration=2.399933, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=124, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74380
[2025-02-27T14:40:40.655-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_payments, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:31.260181+00:00, run_end_date=2025-02-27 17:40:33.551156+00:00, run_duration=2.290975, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=125, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74395
[2025-02-27T14:40:40.656-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_products, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:38.227140+00:00, run_end_date=2025-02-27 17:40:40.459552+00:00, run_duration=2.232412, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=127, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74421
[2025-02-27T14:40:40.687-0300] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:40.688-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:40.689-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:40:40.689-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:40:40.690-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:40.691-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:40.692-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:40:40.693-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:40.694-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:40:40.694-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:40.695-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:40:40.696-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:40.697-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:41.596-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:41.656-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:44.339-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:45.226-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:45.286-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:47.898-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:48.867-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:48.925-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:51.543-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:51.545-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:51.546-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:51.550-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_customers, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:41.721764+00:00, run_end_date=2025-02-27 17:40:44.153508+00:00, run_duration=2.431744, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=128, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:40:40.691371+00:00, queued_by_job_id=120, pid=74432
[2025-02-27T14:40:51.551-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_orders, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:45.353543+00:00, run_end_date=2025-02-27 17:40:47.740098+00:00, run_duration=2.386555, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=129, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:40:40.691371+00:00, queued_by_job_id=120, pid=74444
[2025-02-27T14:40:51.552-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_products, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:48.989940+00:00, run_end_date=2025-02-27 17:40:51.362009+00:00, run_duration=2.372069, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=130, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:40:40.691371+00:00, queued_by_job_id=120, pid=74458
[2025-02-27T14:40:52.772-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:52.773-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:52.774-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:52.775-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:52.776-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-27T14:40:52.776-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:52.778-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:53.620-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:53.679-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:56.174-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:56.179-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_gold_model, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:53.748247+00:00, run_end_date=2025-02-27 17:40:55.992938+00:00, run_duration=2.244691, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=131, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-27 17:40:52.774923+00:00, queued_by_job_id=120, pid=74472
[2025-02-27T14:40:57.627-0300] {dagrun.py:854} INFO - Marking run <DagRun dbt_complete_pipeline @ 2025-02-27 17:40:17.903304+00:00: manual__2025-02-27T17:40:17.903304+00:00, state:running, queued_at: 2025-02-27 17:40:17.910226+00:00. externally triggered: True> successful
[2025-02-27T14:40:57.628-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_complete_pipeline, execution_date=2025-02-27 17:40:17.903304+00:00, run_id=manual__2025-02-27T17:40:17.903304+00:00, run_start_date=2025-02-27 17:40:18.212696+00:00, run_end_date=2025-02-27 17:40:57.628687+00:00, run_duration=39.415991, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-26 01:00:00+00:00, data_interval_end=2025-02-27 01:00:00+00:00, dag_hash=76ce990144eed5917f6427f4f994ba0a
[2025-02-27T14:45:09.116-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-27T14:49:58.928-0300] {scheduler_job_runner.py:435} INFO - 4 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:49:58.930-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:49:58.931-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:49:58.932-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:49:58.933-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 3/10 running and queued tasks
[2025-02-27T14:49:58.934-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:49:58.935-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:49:58.936-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 16 and queue default
[2025-02-27T14:49:58.937-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:49:58.938-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:49:58.939-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:49:58.940-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:49:58.941-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:49:58.941-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:49:58.942-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:49:58.944-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:49:59.816-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:49:59.880-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:00.393-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:01.229-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:01.290-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:04.414-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:05.196-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:05.253-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:07.910-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:08.767-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:08.829-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:11.364-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:11.365-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:11.366-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:11.367-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:11.371-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=clear_db_locks, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:49:59.945245+00:00, run_end_date=2025-02-27 17:50:00.191697+00:00, run_duration=0.246452, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=132, pool=default_pool, queue=default, priority_weight=16, operator=PythonOperator, queued_dttm=2025-02-27 17:49:58.935107+00:00, queued_by_job_id=120, pid=75440
[2025-02-27T14:50:11.373-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_orders, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:01.359897+00:00, run_end_date=2025-02-27 17:50:04.218293+00:00, run_duration=2.858396, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=133, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:49:58.935107+00:00, queued_by_job_id=120, pid=75448
[2025-02-27T14:50:11.373-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_payments, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:05.322320+00:00, run_end_date=2025-02-27 17:50:07.670967+00:00, run_duration=2.348647, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=134, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:49:58.935107+00:00, queued_by_job_id=120, pid=75462
[2025-02-27T14:50:11.375-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_products, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:08.900388+00:00, run_end_date=2025-02-27 17:50:11.173467+00:00, run_duration=2.273079, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=135, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:49:58.935107+00:00, queued_by_job_id=120, pid=75474
[2025-02-27T14:50:11.392-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-27T14:50:12.606-0300] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:12.607-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:50:12.608-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:50:12.608-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:50:12.609-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:12.611-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:50:12.611-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 15 and queue default
[2025-02-27T14:50:12.612-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:12.613-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:50:12.613-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:12.614-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:50:12.615-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:12.616-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:13.403-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:13.462-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:14.965-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:15.904-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:16.017-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:18.561-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:19.396-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:19.457-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:22.029-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:22.030-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:22.031-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:22.035-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_orders, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:16.084905+00:00, run_end_date=2025-02-27 17:50:18.370882+00:00, run_duration=2.285977, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=137, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:50:12.610558+00:00, queued_by_job_id=120, pid=75499
[2025-02-27T14:50:22.036-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_populate_raw_orders, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:13.530610+00:00, run_end_date=2025-02-27 17:50:14.803516+00:00, run_duration=1.272906, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=136, pool=default_pool, queue=default, priority_weight=15, operator=BashOperator, queued_dttm=2025-02-27 17:50:12.610558+00:00, queued_by_job_id=120, pid=75490
[2025-02-27T14:50:22.037-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_products, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:19.524366+00:00, run_end_date=2025-02-27 17:50:21.846866+00:00, run_duration=2.3225, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=138, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:50:12.610558+00:00, queued_by_job_id=120, pid=75513
[2025-02-27T14:50:22.129-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:22.130-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:50:22.131-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:22.132-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:50:22.133-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 14 and queue default
[2025-02-27T14:50:22.134-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:22.135-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:22.987-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:23.047-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:25.749-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:25.754-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_seeds, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:23.118364+00:00, run_end_date=2025-02-27 17:50:25.516885+00:00, run_duration=2.398521, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=139, pool=default_pool, queue=default, priority_weight=14, operator=BashOperator, queued_dttm=2025-02-27 17:50:22.132242+00:00, queued_by_job_id=120, pid=75528
[2025-02-27T14:50:27.251-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:27.252-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:50:27.253-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:27.254-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:50:27.254-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='wait_for_new_customer_data', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:50:27.255-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'wait_for_new_customer_data', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:27.256-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'wait_for_new_customer_data', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:28.158-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:28.217-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:28.579-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='wait_for_new_customer_data', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:28.583-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=wait_for_new_customer_data, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:28.282989+00:00, run_end_date=2025-02-27 17:50:28.359978+00:00, run_duration=0.076989, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=140, pool=default_pool, queue=default, priority_weight=13, operator=FileSensor, queued_dttm=2025-02-27 17:50:27.253796+00:00, queued_by_job_id=120, pid=75548
[2025-02-27T14:50:28.615-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:28.616-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:50:28.617-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:28.618-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:50:28.619-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:50:28.619-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:28.620-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:29.489-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:29.550-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:32.428-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:32.433-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_customers, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:29.623604+00:00, run_end_date=2025-02-27 17:50:32.244961+00:00, run_duration=2.621357, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=141, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:50:28.617979+00:00, queued_by_job_id=120, pid=75551
[2025-02-27T14:50:32.461-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:32.462-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:50:32.463-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:32.464-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:50:32.465-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:50:32.466-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:32.467-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:33.322-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:33.382-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:35.950-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:35.954-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_customers, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:33.450121+00:00, run_end_date=2025-02-27 17:50:35.782654+00:00, run_duration=2.332533, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=142, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:50:32.464236+00:00, queued_by_job_id=120, pid=75565
[2025-02-27T14:50:36.027-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:36.029-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:50:36.029-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>
[2025-02-27T14:50:36.031-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:49:57.097162+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:50:36.032-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-27T14:50:36.033-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:36.035-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:49:57.097162+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:50:36.984-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:50:37.044-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:49:57.097162+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:50:39.670-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:49:57.097162+00:00', try_number=1, map_index=-1)
[2025-02-27T14:50:39.676-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_gold_model, run_id=manual__2025-02-27T17:49:57.097162+00:00, map_index=-1, run_start_date=2025-02-27 17:50:37.107671+00:00, run_end_date=2025-02-27 17:50:39.476513+00:00, run_duration=2.368842, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=143, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-27 17:50:36.031122+00:00, queued_by_job_id=120, pid=75579
[2025-02-27T14:50:39.697-0300] {dagrun.py:854} INFO - Marking run <DagRun dbt_complete_pipeline @ 2025-02-27 17:49:57.097162+00:00: manual__2025-02-27T17:49:57.097162+00:00, state:running, queued_at: 2025-02-27 17:49:57.108840+00:00. externally triggered: True> successful
[2025-02-27T14:50:39.698-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_complete_pipeline, execution_date=2025-02-27 17:49:57.097162+00:00, run_id=manual__2025-02-27T17:49:57.097162+00:00, run_start_date=2025-02-27 17:49:58.912922+00:00, run_end_date=2025-02-27 17:50:39.698639+00:00, run_duration=40.785717, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-26 01:00:00+00:00, data_interval_end=2025-02-27 01:00:00+00:00, dag_hash=fde48f9a125d698b5372b4d670c40e86
[2025-02-27T14:52:24.977-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:24.979-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:52:24.980-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:24.982-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:52:24.983-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 48 and queue default
[2025-02-27T14:52:24.984-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:24.986-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:26.065-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:26.130-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:26.683-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:26.689-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=clear_db_locks, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:26.211390+00:00, run_end_date=2025-02-27 17:52:26.488826+00:00, run_duration=0.277436, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=144, pool=default_pool, queue=default, priority_weight=48, operator=PythonOperator, queued_dttm=2025-02-27 17:52:24.981363+00:00, queued_by_job_id=120, pid=75784
[2025-02-27T14:52:26.717-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:26.718-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:52:26.719-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:26.720-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:52:26.721-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 47 and queue default
[2025-02-27T14:52:26.722-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:26.723-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:27.747-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:27.805-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:29.310-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:29.315-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_populate_raw_orders, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:27.871993+00:00, run_end_date=2025-02-27 17:52:29.126122+00:00, run_duration=1.254129, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=145, pool=default_pool, queue=default, priority_weight=47, operator=BashOperator, queued_dttm=2025-02-27 17:52:26.720457+00:00, queued_by_job_id=120, pid=75791
[2025-02-27T14:52:29.354-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:29.355-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:52:29.356-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:29.358-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:52:29.359-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 46 and queue default
[2025-02-27T14:52:29.359-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:29.360-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:30.266-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:30.326-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:33.364-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:33.369-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_seeds, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:30.391868+00:00, run_end_date=2025-02-27 17:52:33.170897+00:00, run_duration=2.779029, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=146, pool=default_pool, queue=default, priority_weight=46, operator=BashOperator, queued_dttm=2025-02-27 17:52:29.357740+00:00, queued_by_job_id=120, pid=75798
[2025-02-27T14:52:34.569-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:34.570-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:52:34.571-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:34.572-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:52:34.573-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='wait_for_new_customer_data', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 45 and queue default
[2025-02-27T14:52:34.574-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'wait_for_new_customer_data', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:34.575-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'wait_for_new_customer_data', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:35.481-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:35.546-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:35.986-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='wait_for_new_customer_data', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:35.990-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=wait_for_new_customer_data, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:35.614647+00:00, run_end_date=2025-02-27 17:52:35.778150+00:00, run_duration=0.163503, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=147, pool=default_pool, queue=default, priority_weight=45, operator=FileSensor, queued_dttm=2025-02-27 17:52:34.572404+00:00, queued_by_job_id=120, pid=75815
[2025-02-27T14:52:37.278-0300] {scheduler_job_runner.py:435} INFO - 4 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:37.279-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:52:37.279-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:52:37.280-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:52:37.281-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 3/10 running and queued tasks
[2025-02-27T14:52:37.281-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:37.283-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:52:37.283-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:52:37.284-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:37.285-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:52:37.285-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:37.286-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:52:37.287-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:37.288-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:52:37.288-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:37.290-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:38.129-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:38.188-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:40.772-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:41.619-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:41.678-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:44.246-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:45.161-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:45.220-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:47.796-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:48.658-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:48.718-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:51.302-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:51.303-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:51.304-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:51.304-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:52:51.309-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_customers, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:45.281820+00:00, run_end_date=2025-02-27 17:52:47.608520+00:00, run_duration=2.3267, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=150, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:52:37.282679+00:00, queued_by_job_id=120, pid=75858
[2025-02-27T14:52:51.310-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_orders, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:38.247071+00:00, run_end_date=2025-02-27 17:52:40.593117+00:00, run_duration=2.346046, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=148, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:52:37.282679+00:00, queued_by_job_id=120, pid=75823
[2025-02-27T14:52:51.311-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_payments, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:41.744832+00:00, run_end_date=2025-02-27 17:52:43.995566+00:00, run_duration=2.250734, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=149, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:52:37.282679+00:00, queued_by_job_id=120, pid=75843
[2025-02-27T14:52:51.312-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_products, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:48.781511+00:00, run_end_date=2025-02-27 17:52:51.120480+00:00, run_duration=2.338969, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=151, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:52:37.282679+00:00, queued_by_job_id=120, pid=75870
[2025-02-27T14:52:51.379-0300] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:51.380-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:52:51.380-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:52:51.381-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:52:51.382-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:52:51.383-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:52:51.383-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:52:51.384-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:51.385-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:52:51.385-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:51.386-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:52:51.386-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:51.388-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:52.211-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:52.286-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:54.933-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:55.817-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:55.875-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:52:58.730-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:52:59.811-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:52:59.871-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:53:02.420-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:53:02.422-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:53:02.423-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:53:02.428-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_customers, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:52.346363+00:00, run_end_date=2025-02-27 17:52:54.739299+00:00, run_duration=2.392936, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=152, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:52:51.382694+00:00, queued_by_job_id=120, pid=75903
[2025-02-27T14:53:02.429-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_orders, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:55.939522+00:00, run_end_date=2025-02-27 17:52:58.534212+00:00, run_duration=2.59469, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=153, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:52:51.382694+00:00, queued_by_job_id=120, pid=75927
[2025-02-27T14:53:02.430-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_products, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:52:59.940537+00:00, run_end_date=2025-02-27 17:53:02.231277+00:00, run_duration=2.29074, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=154, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:52:51.382694+00:00, queued_by_job_id=120, pid=75939
[2025-02-27T14:53:02.499-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:53:02.500-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:53:02.500-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>
[2025-02-27T14:53:02.502-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:52:23.043718+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:53:02.503-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-27T14:53:02.504-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:53:02.505-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:52:23.043718+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:53:03.473-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:53:03.533-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:52:23.043718+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:53:06.018-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:52:23.043718+00:00', try_number=1, map_index=-1)
[2025-02-27T14:53:06.023-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_gold_model, run_id=manual__2025-02-27T17:52:23.043718+00:00, map_index=-1, run_start_date=2025-02-27 17:53:03.599118+00:00, run_end_date=2025-02-27 17:53:05.859466+00:00, run_duration=2.260348, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=155, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-27 17:53:02.501970+00:00, queued_by_job_id=120, pid=75949
[2025-02-27T14:53:07.148-0300] {dagrun.py:854} INFO - Marking run <DagRun dbt_complete_pipeline @ 2025-02-27 17:52:23.043718+00:00: manual__2025-02-27T17:52:23.043718+00:00, state:running, queued_at: 2025-02-27 17:52:23.054514+00:00. externally triggered: True> successful
[2025-02-27T14:53:07.149-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_complete_pipeline, execution_date=2025-02-27 17:52:23.043718+00:00, run_id=manual__2025-02-27T17:52:23.043718+00:00, run_start_date=2025-02-27 17:52:24.946198+00:00, run_end_date=2025-02-27 17:53:07.149049+00:00, run_duration=42.202851, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-26 01:00:00+00:00, data_interval_end=2025-02-27 01:00:00+00:00, dag_hash=1749e11bea079b51c95da25dcba3cfa8
[2025-02-27T14:55:11.436-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-27T14:56:47.688-0300] {scheduler_job_runner.py:435} INFO - 5 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:56:47.690-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:56:47.691-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:56:47.692-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:56:47.693-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 3/10 running and queued tasks
[2025-02-27T14:56:47.694-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 4/10 running and queued tasks
[2025-02-27T14:56:47.695-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:56:47.697-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:56:47.699-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 14 and queue default
[2025-02-27T14:56:47.700-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:47.701-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:56:47.702-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:47.704-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:56:47.705-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:47.706-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:56:47.708-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:47.709-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-27T14:56:47.710-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:47.744-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:48.658-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:56:48.718-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:56:49.256-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:50.325-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:56:50.390-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:56:53.197-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:54.181-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:56:54.240-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:56:56.750-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:56:57.653-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:56:57.714-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:00.102-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:00.941-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:57:01.001-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:02.521-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:02.523-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:02.524-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:02.524-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:02.525-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:02.564-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=clear_db_locks, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:56:48.786914+00:00, run_end_date=2025-02-27 17:56:49.058478+00:00, run_duration=0.271564, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=156, pool=default_pool, queue=default, priority_weight=14, operator=PythonOperator, queued_dttm=2025-02-27 17:56:47.696646+00:00, queued_by_job_id=120, pid=76462
[2025-02-27T14:57:02.565-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_populate_raw_orders, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:57:01.070455+00:00, run_end_date=2025-02-27 17:57:02.307438+00:00, run_duration=1.236983, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=160, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-27 17:56:47.696646+00:00, queued_by_job_id=120, pid=76505
[2025-02-27T14:57:02.567-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_orders, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:56:50.456506+00:00, run_end_date=2025-02-27 17:56:53.022142+00:00, run_duration=2.565636, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=157, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:56:47.696646+00:00, queued_by_job_id=120, pid=76469
[2025-02-27T14:57:02.568-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_payments, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:56:54.305180+00:00, run_end_date=2025-02-27 17:56:56.566335+00:00, run_duration=2.261155, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=158, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:56:47.696646+00:00, queued_by_job_id=120, pid=76481
[2025-02-27T14:57:02.569-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_products, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:56:57.775660+00:00, run_end_date=2025-02-27 17:56:59.916591+00:00, run_duration=2.140931, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=159, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:56:47.696646+00:00, queued_by_job_id=120, pid=76493
[2025-02-27T14:57:03.923-0300] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:03.924-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:57:03.924-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:57:03.925-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:57:03.925-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:03.927-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:57:03.927-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='wait_for_new_customer_data', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:57:03.928-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'wait_for_new_customer_data', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:03.929-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:57:03.930-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:03.931-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:57:03.931-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:03.933-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'wait_for_new_customer_data', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:04.762-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:57:04.821-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.wait_for_new_customer_data manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:05.176-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:06.175-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:57:06.236-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:08.787-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:09.589-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:57:09.650-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:12.145-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='wait_for_new_customer_data', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:12.148-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:12.148-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:12.152-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_orders, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:57:06.305794+00:00, run_end_date=2025-02-27 17:57:08.601532+00:00, run_duration=2.295738, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=162, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:57:03.926742+00:00, queued_by_job_id=120, pid=76521
[2025-02-27T14:57:12.153-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_products, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:57:09.722600+00:00, run_end_date=2025-02-27 17:57:11.958366+00:00, run_duration=2.235766, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=163, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:57:03.926742+00:00, queued_by_job_id=120, pid=76533
[2025-02-27T14:57:12.154-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=wait_for_new_customer_data, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:57:04.887368+00:00, run_end_date=2025-02-27 17:57:04.960377+00:00, run_duration=0.073009, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=161, pool=default_pool, queue=default, priority_weight=13, operator=FileSensor, queued_dttm=2025-02-27 17:57:03.926742+00:00, queued_by_job_id=120, pid=76516
[2025-02-27T14:57:12.217-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:12.218-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:57:12.219-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:12.220-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:57:12.221-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:57:12.222-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:12.223-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:13.028-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:57:13.111-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:15.727-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:15.731-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_customers, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:57:13.174264+00:00, run_end_date=2025-02-27 17:57:15.518566+00:00, run_duration=2.344302, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=164, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:57:12.220227+00:00, queued_by_job_id=120, pid=76544
[2025-02-27T14:57:15.761-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:15.762-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:57:15.763-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:15.764-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:57:15.764-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:57:15.765-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:15.766-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:16.927-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:57:16.995-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:19.766-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:19.773-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_customers, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:57:17.059783+00:00, run_end_date=2025-02-27 17:57:19.544062+00:00, run_duration=2.484279, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=165, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:57:15.763786+00:00, queued_by_job_id=120, pid=76559
[2025-02-27T14:57:20.985-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:20.986-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:57:20.987-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>
[2025-02-27T14:57:20.988-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:56:46.993013+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:57:20.989-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-27T14:57:20.990-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:20.991-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:56:46.993013+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:57:22.110-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:57:22.169-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:56:46.993013+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:57:24.886-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:56:46.993013+00:00', try_number=1, map_index=-1)
[2025-02-27T14:57:24.890-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_gold_model, run_id=manual__2025-02-27T17:56:46.993013+00:00, map_index=-1, run_start_date=2025-02-27 17:57:22.239568+00:00, run_end_date=2025-02-27 17:57:24.692649+00:00, run_duration=2.453081, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=166, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-27 17:57:20.988163+00:00, queued_by_job_id=120, pid=76580
[2025-02-27T14:57:24.916-0300] {dagrun.py:854} INFO - Marking run <DagRun dbt_complete_pipeline @ 2025-02-27 17:56:46.993013+00:00: manual__2025-02-27T17:56:46.993013+00:00, state:running, queued_at: 2025-02-27 17:56:47.009280+00:00. externally triggered: True> successful
[2025-02-27T14:57:24.917-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_complete_pipeline, execution_date=2025-02-27 17:56:46.993013+00:00, run_id=manual__2025-02-27T17:56:46.993013+00:00, run_start_date=2025-02-27 17:56:47.640328+00:00, run_end_date=2025-02-27 17:57:24.917818+00:00, run_duration=37.27749, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-26 01:00:00+00:00, data_interval_end=2025-02-27 01:00:00+00:00, dag_hash=aa61464928d5bc2bda59d86c116bec5a
[2025-02-27T14:59:14.732-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:14.733-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/16 running and queued tasks
[2025-02-27T14:59:14.734-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:14.736-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:59:14.737-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 11 and queue default
[2025-02-27T14:59:14.738-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:14.740-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:15.617-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:15.676-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:16.139-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:16.144-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=clear_db_locks, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:15.743046+00:00, run_end_date=2025-02-27 17:59:15.936390+00:00, run_duration=0.193344, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=167, pool=default_pool, queue=default, priority_weight=11, operator=PythonOperator, queued_dttm=2025-02-27 17:59:14.736227+00:00, queued_by_job_id=120, pid=76840
[2025-02-27T14:59:16.172-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:16.173-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/16 running and queued tasks
[2025-02-27T14:59:16.174-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:16.175-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:59:16.176-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 10 and queue default
[2025-02-27T14:59:16.177-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:16.178-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:17.154-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:17.213-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:18.534-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:18.539-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_populate_raw_orders, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:17.278614+00:00, run_end_date=2025-02-27 17:59:18.346004+00:00, run_duration=1.06739, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=168, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-02-27 17:59:16.175352+00:00, queued_by_job_id=120, pid=76848
[2025-02-27T14:59:18.569-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:18.569-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/16 running and queued tasks
[2025-02-27T14:59:18.570-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:18.571-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:59:18.572-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 9 and queue default
[2025-02-27T14:59:18.573-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:18.574-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:19.466-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:19.525-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:22.343-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:22.348-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_seeds, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:19.592754+00:00, run_end_date=2025-02-27 17:59:22.186077+00:00, run_duration=2.593323, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=169, pool=default_pool, queue=default, priority_weight=9, operator=BashOperator, queued_dttm=2025-02-27 17:59:18.571358+00:00, queued_by_job_id=120, pid=76856
[2025-02-27T14:59:22.375-0300] {scheduler_job_runner.py:435} INFO - 4 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:22.376-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/16 running and queued tasks
[2025-02-27T14:59:22.377-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/16 running and queued tasks
[2025-02-27T14:59:22.377-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/16 running and queued tasks
[2025-02-27T14:59:22.378-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 3/16 running and queued tasks
[2025-02-27T14:59:22.378-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:22.379-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:59:22.380-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 4 and queue default
[2025-02-27T14:59:22.381-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:22.381-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 4 and queue default
[2025-02-27T14:59:22.382-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:22.383-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-27T14:59:22.384-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:22.384-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
[2025-02-27T14:59:22.385-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:22.386-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:23.339-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:23.402-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:26.121-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:27.023-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:27.081-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:29.665-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:30.520-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:30.580-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:33.355-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:34.207-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:34.265-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:37.028-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:37.031-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:37.031-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:37.033-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:37.039-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_customers, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:30.648257+00:00, run_end_date=2025-02-27 17:59:33.173582+00:00, run_duration=2.525325, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=172, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2025-02-27 17:59:22.379494+00:00, queued_by_job_id=120, pid=76895
[2025-02-27T14:59:37.040-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_orders, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:23.477105+00:00, run_end_date=2025-02-27 17:59:25.937763+00:00, run_duration=2.460658, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=170, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-02-27 17:59:22.379494+00:00, queued_by_job_id=120, pid=76874
[2025-02-27T14:59:37.041-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_payments, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:27.152368+00:00, run_end_date=2025-02-27 17:59:29.416179+00:00, run_duration=2.263811, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=171, pool=default_pool, queue=default, priority_weight=4, operator=BashOperator, queued_dttm=2025-02-27 17:59:22.379494+00:00, queued_by_job_id=120, pid=76886
[2025-02-27T14:59:37.042-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_products, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:34.329947+00:00, run_end_date=2025-02-27 17:59:36.833394+00:00, run_duration=2.503447, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=173, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2025-02-27 17:59:22.379494+00:00, queued_by_job_id=120, pid=76910
[2025-02-27T14:59:37.079-0300] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:37.079-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/16 running and queued tasks
[2025-02-27T14:59:37.080-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/16 running and queued tasks
[2025-02-27T14:59:37.081-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/16 running and queued tasks
[2025-02-27T14:59:37.082-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:37.083-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:59:37.084-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:59:37.084-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:37.085-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:59:37.086-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:37.086-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:59:37.087-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:37.088-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:38.014-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:38.077-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:40.732-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:41.704-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:41.776-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:44.375-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:45.236-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:45.297-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:47.865-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:47.866-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:47.867-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:47.871-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_customers, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:38.140461+00:00, run_end_date=2025-02-27 17:59:40.544048+00:00, run_duration=2.403587, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=174, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:59:37.082878+00:00, queued_by_job_id=120, pid=76926
[2025-02-27T14:59:47.873-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_orders, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:41.841785+00:00, run_end_date=2025-02-27 17:59:44.208938+00:00, run_duration=2.367153, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=175, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:59:37.082878+00:00, queued_by_job_id=120, pid=76956
[2025-02-27T14:59:47.874-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_products, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:45.365575+00:00, run_end_date=2025-02-27 17:59:47.715474+00:00, run_duration=2.349899, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=176, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:59:37.082878+00:00, queued_by_job_id=120, pid=76969
[2025-02-27T14:59:49.012-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:49.013-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/16 running and queued tasks
[2025-02-27T14:59:49.014-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>
[2025-02-27T14:59:49.015-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:59:13.407128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:59:49.016-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-27T14:59:49.016-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:49.018-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:59:13.407128+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:59:49.858-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:59:49.916-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:59:13.407128+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:59:52.765-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:59:13.407128+00:00', try_number=1, map_index=-1)
[2025-02-27T14:59:52.770-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_gold_model, run_id=manual__2025-02-27T17:59:13.407128+00:00, map_index=-1, run_start_date=2025-02-27 17:59:49.982144+00:00, run_end_date=2025-02-27 17:59:52.519613+00:00, run_duration=2.537469, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=177, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-27 17:59:49.015030+00:00, queued_by_job_id=120, pid=76983
[2025-02-27T14:59:54.313-0300] {dagrun.py:854} INFO - Marking run <DagRun dbt_complete_pipeline @ 2025-02-27 17:59:13.407128+00:00: manual__2025-02-27T17:59:13.407128+00:00, state:running, queued_at: 2025-02-27 17:59:13.417583+00:00. externally triggered: True> successful
[2025-02-27T14:59:54.314-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_complete_pipeline, execution_date=2025-02-27 17:59:13.407128+00:00, run_id=manual__2025-02-27T17:59:13.407128+00:00, run_start_date=2025-02-27 17:59:14.722331+00:00, run_end_date=2025-02-27 17:59:54.314029+00:00, run_duration=39.591698, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-26 01:00:00+00:00, data_interval_end=2025-02-27 01:00:00+00:00, dag_hash=c0ca33743d2e53c034c87b369a9565f8
[2025-02-27T15:00:11.471-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-27T15:05:11.512-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-27T15:10:11.559-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
