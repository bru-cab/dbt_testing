[2025-02-27T14:40:08.864-0300] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2025-02-27T14:40:08.973-0300] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2025-02-27T14:40:08.974-0300] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2025-02-27T14:40:09.028-0300] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 74312
[2025-02-27T14:40:09.030-0300] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2025-02-27T14:40:10.024-0300] {settings.py:63} INFO - Configured default timezone UTC
[2025-02-27T14:40:10.037-0300] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2025-02-27T14:40:18.266-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:18.267-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:18.268-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:18.270-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:18.271-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 47 and queue default
[2025-02-27T14:40:18.272-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:18.273-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'clear_db_locks', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:19.213-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:19.279-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.clear_db_locks manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:19.791-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='clear_db_locks', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:19.797-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=clear_db_locks, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:19.342782+00:00, run_end_date=2025-02-27 17:40:19.637293+00:00, run_duration=0.294511, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=121, pool=default_pool, queue=default, priority_weight=47, operator=PythonOperator, queued_dttm=2025-02-27 17:40:18.269652+00:00, queued_by_job_id=120, pid=74350
[2025-02-27T14:40:19.830-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:19.831-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:19.832-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:19.833-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:19.834-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 46 and queue default
[2025-02-27T14:40:19.834-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:19.836-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_populate_raw_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:20.719-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:20.777-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_populate_raw_orders manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:22.153-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_populate_raw_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:22.157-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_populate_raw_orders, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:20.843892+00:00, run_end_date=2025-02-27 17:40:21.987795+00:00, run_duration=1.143903, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=122, pool=default_pool, queue=default, priority_weight=46, operator=BashOperator, queued_dttm=2025-02-27 17:40:19.832985+00:00, queued_by_job_id=120, pid=74358
[2025-02-27T14:40:22.184-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:22.185-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:22.186-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:22.187-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:22.188-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 45 and queue default
[2025-02-27T14:40:22.189-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:22.190-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_seeds', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:23.077-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:23.136-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_seeds manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:26.488-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_seeds', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:26.493-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_seeds, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:23.205406+00:00, run_end_date=2025-02-27 17:40:26.243570+00:00, run_duration=3.038164, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=123, pool=default_pool, queue=default, priority_weight=45, operator=BashOperator, queued_dttm=2025-02-27 17:40:22.187213+00:00, queued_by_job_id=120, pid=74366
[2025-02-27T14:40:26.524-0300] {scheduler_job_runner.py:435} INFO - 4 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:26.525-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:26.526-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:40:26.526-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:40:26.527-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 3/10 running and queued tasks
[2025-02-27T14:40:26.528-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:26.529-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:26.530-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:40:26.530-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.531-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 13 and queue default
[2025-02-27T14:40:26.532-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.532-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:40:26.533-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.534-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 12 and queue default
[2025-02-27T14:40:26.535-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:26.536-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:27.445-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:27.505-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_orders manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:30.162-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_payments', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:31.133-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:31.192-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_payments manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:33.735-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:34.694-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:34.752-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_customers manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:37.236-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_stg_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:38.095-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:38.155-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_stg_products manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:40.645-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.647-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_payments', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.648-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.649-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_stg_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:40.653-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_customers, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:34.824460+00:00, run_end_date=2025-02-27 17:40:37.054367+00:00, run_duration=2.229907, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=126, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74409
[2025-02-27T14:40:40.654-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_orders, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:27.570806+00:00, run_end_date=2025-02-27 17:40:29.970739+00:00, run_duration=2.399933, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=124, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74380
[2025-02-27T14:40:40.655-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_payments, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:31.260181+00:00, run_end_date=2025-02-27 17:40:33.551156+00:00, run_duration=2.290975, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=125, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74395
[2025-02-27T14:40:40.656-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_stg_products, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:38.227140+00:00, run_end_date=2025-02-27 17:40:40.459552+00:00, run_duration=2.232412, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=127, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-02-27 17:40:26.528977+00:00, queued_by_job_id=120, pid=74421
[2025-02-27T14:40:40.687-0300] {scheduler_job_runner.py:435} INFO - 3 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:40.688-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:40.689-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 1/10 running and queued tasks
[2025-02-27T14:40:40.689-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 2/10 running and queued tasks
[2025-02-27T14:40:40.690-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
	<TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:40.691-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>, <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:40.692-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:40:40.693-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:40.694-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:40:40.694-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:40.695-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2025-02-27T14:40:40.696-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:40.697-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_customers', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:41.596-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:41.656-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_customers manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:44.339-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_orders', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:45.226-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:45.286-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_orders manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:47.898-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_products', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:48.867-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:48.925-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_products manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:51.543-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_customers', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:51.545-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_orders', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:51.546-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_products', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:51.550-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_customers, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:41.721764+00:00, run_end_date=2025-02-27 17:40:44.153508+00:00, run_duration=2.431744, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=128, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:40:40.691371+00:00, queued_by_job_id=120, pid=74432
[2025-02-27T14:40:51.551-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_orders, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:45.353543+00:00, run_end_date=2025-02-27 17:40:47.740098+00:00, run_duration=2.386555, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=129, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:40:40.691371+00:00, queued_by_job_id=120, pid=74444
[2025-02-27T14:40:51.552-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_products, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:48.989940+00:00, run_end_date=2025-02-27 17:40:51.362009+00:00, run_duration=2.372069, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=130, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2025-02-27 17:40:40.691371+00:00, queued_by_job_id=120, pid=74458
[2025-02-27T14:40:52.772-0300] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:52.773-0300] {scheduler_job_runner.py:507} INFO - DAG dbt_complete_pipeline has 0/10 running and queued tasks
[2025-02-27T14:40:52.774-0300] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>
[2025-02-27T14:40:52.775-0300] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2025-02-27T14:40:52.776-0300] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2025-02-27T14:40:52.776-0300] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:52.778-0300] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'dbt_complete_pipeline', 'run_dbt_gold_model', 'manual__2025-02-27T17:40:17.903304+00:00', '--local', '--subdir', 'DAGS_FOLDER/dbt_complete_pipeline_dag.py']
[2025-02-27T14:40:53.620-0300] {dagbag.py:588} INFO - Filling up the DagBag from /Users/bruno/dbt_testing/airflow/dags/dbt_complete_pipeline_dag.py
[2025-02-27T14:40:53.679-0300] {task_command.py:467} INFO - Running <TaskInstance: dbt_complete_pipeline.run_dbt_gold_model manual__2025-02-27T17:40:17.903304+00:00 [queued]> on host 192.168.1.95
[2025-02-27T14:40:56.174-0300] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dbt_complete_pipeline', task_id='run_dbt_gold_model', run_id='manual__2025-02-27T17:40:17.903304+00:00', try_number=1, map_index=-1)
[2025-02-27T14:40:56.179-0300] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=dbt_complete_pipeline, task_id=run_dbt_gold_model, run_id=manual__2025-02-27T17:40:17.903304+00:00, map_index=-1, run_start_date=2025-02-27 17:40:53.748247+00:00, run_end_date=2025-02-27 17:40:55.992938+00:00, run_duration=2.244691, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=2, job_id=131, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-02-27 17:40:52.774923+00:00, queued_by_job_id=120, pid=74472
[2025-02-27T14:40:57.627-0300] {dagrun.py:854} INFO - Marking run <DagRun dbt_complete_pipeline @ 2025-02-27 17:40:17.903304+00:00: manual__2025-02-27T17:40:17.903304+00:00, state:running, queued_at: 2025-02-27 17:40:17.910226+00:00. externally triggered: True> successful
[2025-02-27T14:40:57.628-0300] {dagrun.py:905} INFO - DagRun Finished: dag_id=dbt_complete_pipeline, execution_date=2025-02-27 17:40:17.903304+00:00, run_id=manual__2025-02-27T17:40:17.903304+00:00, run_start_date=2025-02-27 17:40:18.212696+00:00, run_end_date=2025-02-27 17:40:57.628687+00:00, run_duration=39.415991, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-02-26 01:00:00+00:00, data_interval_end=2025-02-27 01:00:00+00:00, dag_hash=76ce990144eed5917f6427f4f994ba0a
